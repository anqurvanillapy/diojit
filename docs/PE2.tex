\documentclass[12pt, a4paper]{report}

\usepackage[top=3.5cm,bottom=3cm,left=2.5cm,right=2.5cm]{geometry}
\usepackage{parallel, amsfonts, amsmath, amsthm, amssymb}
\usepackage[epsilon, tsrm, altpo]{backnaur}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{bussproofs}
\lstset{language=Python, showlines=true}
\usepackage{natbib}

\newcommand{\ddfrac}[3]{
    \begin{prooftree}
    \AxiomC{$#1$}
    \AxiomC{$#2$}
    \BinaryInfC{$#3$}
    \end{prooftree}
}

\newcommand{\centerfrac}[4]{
    $$ #2 $$
    $$
        \dfrac { #3 } { #4 } #1
    $$
}

\newcommand{\negsp}[1]{
    \!\!#1\!\!
}


\newcommand{\rulegroup}[1]{
    \textbf{#1}
}

\newcommand{\gap}{ \;\;\;\; }
\newcommand{\mbold}[1]{ \mathbf{#1} }

\title{Summary: A Just-In-Time Compiler for Python}

\begin{document}
\maketitle

\section*{Architecture}

We have a global state \textbf{JIT STATE} to maintain JIT compiler-awared code,
generated IRs, the information to tell whether a Python object has been or can be JIT-ed,
and counters for hot paths, etc.

Given the Python source code, the standard compiler of Python will get us the Python bytecode
instructions and associate runtime objects. 

We then convert Python instructions into \textbf{Core CPY}, which is more compact and easier to handle in the following pipeline.

Afterwards, by specifying a global variable context indicating non-volatile global variables,
we can specialize \textbf{Core CPY} into a register based IR, hereafter as \textbf{Dynjit}.

The process of transforming \textbf{Core CPY} into \textbf{Dynjit} is sophiscated and considered as the major part of
our JIT compiler.

This transformation takes advantage of \textbf{JIT STATE} and assumptions of Python builtins,
to statically partial evaluate the programs to produce the register based IR \textbf{Dynjit}.

The top-level partial evaluation happens when calling the "main" function. This does not mean we're restricted to
programs containing a "main" function, but rather indicates that the point in time to perform runtime specialization.
Any function whose invocation triggers runtime specialization can be regarded as a "main" function.

By knowing the argument types of a "main" function, we can perform many kinds of specializations in the program.
The function calls inside "main" function can be specialized while specializing "main" function, in other words,
it is possible to only trigger runtime code generation once to run your program.

However, not every piece of Python code can be efficiently optimized by our JIT compiler.

Changing global variables(\lstinline!global!), changing free variables in nested functions(\lstinline!nonlocal!),
and many other bad practice will not only ruin the readability or maintainability, but also affect the performance of our JIT compiler.

Finally, the JIT compiler is guaranteed to work fine for code written under a good specification, which is given in the Appendix(Appendix is WIP).

\section*{Overview: Capability of the JIT Compiler}

Our JIT compiler mainly does 3 kinds of specializations.

\begin{itemize}
    \item Method lookup specializations
    \item Control flow specializations
    \item Union splitting specializations
\end{itemize}

\subsubsection*{Method Lookup Specializations}

Given the following code, and use function \textit{f} as a "main" function.

\begin{lstlisting}

class S:
def f(self):
    ...

def f(x):
    return x.f()

# 'f' is a 'main'
f(S())

\end{lstlisting}


After partial evaluation for \textit{f(S())} , we can know that the argument \textit{x} has type \textit{S}, hence we can load the method object \textit{S.f} as a constant,
and call \textit{S.f(x)}.

Note that in standard Python interpreter, every time accessing a method or field will constantly lookup hash tables.

By gathering method lookup specialization and Python-to-C compilation, we can eliminate the overhead of method lookup.

Performance gain of this optimization is more significant when it comes to larger inheritance chains.

\subsubsection*{Control Flow Specializations}

\begin{lstlisting}
def f(x):
    if isinstance(x, int):
        return x + 10

    return 0
f(1)

\end{lstlisting}

Above code shows a case that the condition expression of if-else is specialize-able.

By giving the correct global variable context, we can recognize Python built-ins from the symbol \textit{int} and \textit{isinstance}.

Under this situation, our compiler can partial evaluate \textit{isinstance(x, int)} to a constant \textit{True}, and then
partial evaluate \textit{if True}, statically select the first arm and eliminate the redundant branch.

So the function call \textit{f(1)} becomes in fact something like \textit{f\_int(1)}, where

\begin{lstlisting}

def f_int(x: int):
    return x + 10

\end{lstlisting}

Of course, the specialized code is generated C code. Above Python code about \textit{f\_int} is made for understandability.

\subsubsection*{Union Splitting Specializations}

Sometimes an expression can hold union types.

\begin{lstlisting}
    
def one_of_string(x):
    if x == "int":
        return 1
    elif x == "float":
        return 1.0
    elif x == "str":
        return "_"
    else:
        raise ValueError

def two_of_string(x):
    x = zero_of_string(x)
    return x + x


two_of_string("int")

\end{lstlisting}

The "main" function here is \textit{two\_of\_string}, however, the argument type is simply a \textit{str},
and the JIT compiler presented here does not know the argument value.

So the return \textit{two\_of\_string(x)} will be a union of \textit{int}, \textit{float}, and \textit{str},
which will prevent the specialization of the later expression \textit{x + x}.

To address this, we use the idea "union splitting" from Julia programming language. % \cite{bezanson2017julia}.

After calling a function, the return can have union types.

We will insert code for union splitting specialization to somewhere prior to the first time we use the value of union type as function argument.

In the example of \textit{two\_of\_string(x: str)}, we will specialize it to

\begin{lstlisting}

def two_of_string(x: str):
    x = zero_of_string(x)
    switch type(x):
    case int:
        return plus_int(x, x)
    case float:
        return plus_float(x, x)
    case str:
        return plus_str(x, x)

\end{lstlisting}

We contrive a switch statement for Python, to express the type dispatching semantics of generated C code.

\textit{plus\_*} are the specialized methods for \textbf{+} function. It is worth noting that every operation in Python bytecode is converted to
a function call in \textbf{Core CPY}.


\section*{Core CPY}

Core CPY is a simplification of Python bytecode instructions.

\subsection*{Instructions of Core CPY}

\begin{bnf*}
    \bnfprod{instr}{ \bnftd{Const} \bnfsp \bnftd{$\mathbf{constant}$}}\\
    \bnfmore{\bnfor \bnftd{Load} \bnfsp \bnftd{$\mathbf{symbol}$}}\\
    \bnfmore{\bnfor \bnftd{Store} \bnfsp \bnftd{$\mathbf{symbol}$}}\\
    \bnfmore{\bnfor \bnftd{Jump} \bnfsp \bnftd{$\mathbf{int}$}}\\
    \bnfmore{\bnfor \bnftd{Call} \bnfsp \bnftd{$\mathbf{int}$}}\\
    \bnfmore{\bnfor \bnftd{Rot} \bnfsp \bnftd{$\mathbf{int}$}} \\
    \bnfmore{\bnfor \bnftd{Peek} \bnfsp \bnftd{$\mathbf{int}$}} \\
    \bnfmore{\bnfor \bnftd{Pop}} \\
    \bnfmore{\bnfor \bnftd{Return}} \\
    \bnfmore{\bnfor \bnftd{JumpIf} \bnfsp \bnftd{$\mathbf{bool}$} \bnfsp \bnftd{$\mathbf{bool}$} \bnfsp \bnftd{$\mathbf{int}$}} 
\end{bnf*}

\newpage

\subsection*{Notations and Prerequisites}

The semantics here is not complete, but it is unnecessary to be complete.

Many parts, such as Python data models, function closures, default arguments, keyword arguments, variadic arguments, etc., are omitted in Core CPY.

This is a must because any of those omitted parts will involve a book longer than this document, and it is sufficient to understand
the semantics by the following alternative approach.

Instead of elaborating how values transform with respect to heaps on each instruction of Core CPY,
we gives semantics targeting equivalent Python code. In this regarding, we can use built-ins from Python runtime to
avoid massive but quite uninformative explanations.

Specifically, the term "Python object" used here, specially means a regular value in Python runtime, which is compatible to \textit{PyObject*} in C level.

Following are about the notations we need for the semantics.

Like Python bytecode instructions, Core CPY also leverages a value stack, which can be denoted with $S$.

Besides, we need

\begin{itemize}
    \item $i, i_0, i_1, \cdots$, an integer.
    \item $n, n_0, n_1, \cdots$, a symbol, a name.
    \item $a, a_0, a_1, \cdots$, a Python object.
    \item a sequence of instruction $\mathbf{I}$, can be indexed like $I[i]$.
    \item A map $\Delta$ to map a name $n$ to the index of value stack $\Delta(n)$. This is fixed for every function/instruction sequence.
    \item The side-effect-free substitution operation $\cdot[\cdot \leftarrow \cdot]$.
          Given an offset of value stack $i \in \mathbb{N}$, a Python object $v$,
          $S[i \leftarrow v]$ returns a new stack $S^{'}$ which equals to $S$ except $S[i]$ is $v$.
    \item $\{\cdot\}_{python}$ means evaluating the Python code, and returns a Python object
\end{itemize}

The value stack $S$ is not empty in the beginning. It contains arguments and free variables.

\begin{lstlisting}

def f(x,y,z):
    a = x + 1
    return (a, x)

f(1.0, 2, "3.0")

\end{lstlisting}
The function call \textit{f(1.0)}, will initialize a value stack $[1.0, 2, "3.0"]$.

\subsection*{Semantics}


\hrulefill

\rulegroup{VAR}

\begin{equation*}
\frac{}{S \vdash_{r-val} n \Rightarrow S[\Delta(n)]}
\end{equation*}    

\hrulefill
\bigbreak

\rulegroup{INSTR}

\begin{equation*}
\begin{split}
\frac{
    \_ , \; \overbrace{\cdots}^{S^{'}} = S
}{
    S \vdash_{instr} \mathbf{Pop} \Rightarrow S^{'}
}
\;\;
& \frac{
    a_1, \cdots, a_i, a_{peek}, \cdots = S
}{
    S \vdash_{instr} \mathbf{Peek}\; i \Rightarrow a_{peek}, \; \overbrace{\cdots}^{S}
}
\\\\
\frac{S \vdash_{r-val} n \Rightarrow a}
     {S \vdash_{instr} \mathbf{Load}\; n \Rightarrow a, \; \overbrace{\cdots}^{S} }
\;\;
& \frac{}
     {S \vdash_{instr} \mathbf{Const}\; a \Rightarrow a, \; \overbrace{\cdots}^{S}}
\end{split}
\end{equation*}

\begin{equation*}
\begin{split}
\frac{
    a_1, a_2, \cdots, a_i, \; \overbrace{\cdots}^{S^{'}} = S
}{
    S \vdash_{instr} \mathbf{Rot} \; i \Rightarrow a_i, a_1, a_2, \cdots, a_{i - 1},\; \overbrace{\cdots}^{S^{'}}
}
\\\\
\frac{ a,\; \overbrace{\cdots}^{S^{'}} = S \;\;\;\; i = \Delta(n) }
        { S \vdash_{instr} \mathbf{Store} \; n \Rightarrow S^{'}[i \leftarrow a]}
\end{split}
\end{equation*}


\begin{equation*}
\begin{split}
\frac{
    a_1, a_2 , \cdots, a_n, f, \; \overbrace{\cdots}^{S^{'}} = S
    \gap
    a_{r} = \{f(a_1, a_2, \cdots, a_n)\}_{python}
}{
    S \vdash_{instr} \mathbf{Call}\;n \Rightarrow a_{r}, \; \overbrace{\cdots}^{S^{'}}
}
\end{split}
\end{equation*}

\hrulefill

\rulegroup{JUMP}

\begin{equation*}
\begin{split}
\frac{ \mathbf{Jump} \; \mathit{off} = I[i] \;\;\;\; I, S \vdash_{jump} \mathit{off} \Rightarrow a}
     { I, S \vdash_{jump} i \Rightarrow a}
\end{split}
\end{equation*}


\ddfrac{
    \mathbf{JumpIf} \; b \; False \; \mathit{off} = I[i]
    \gap
    a, \; \overbrace{\cdots}^{S^{'}} = S
}{
    b = \{bool(a)\}_{python}
    \gap
    I, S^{'} \vdash_{jump} \mathit{off} \Rightarrow a_r
}{ I, S \vdash_{jump} i \Rightarrow a_r }

\ddfrac{
    \mathbf{JumpIf} \; b \;False \mathit{off} = I[i]
    \gap
    a, \; \overbrace{\cdots}^{S^{'}} = S
}{
    b \neq \{bool(a)\}_{python}
    \gap
    I, S^{'} \vdash_{jump} \mathit{i}+1 \Rightarrow a_r
}{ I, S \vdash_{jump} i \Rightarrow a_r}

\ddfrac{
    \mathbf{JumpIf} \; b \; True \; \mathit{off} = I[i]
    \gap
    a, \; \cdots = S
}{
    b = \{bool(a)\}_{python}
    \gap
    I, S \vdash_{jump} \mathit{off} \Rightarrow a_r
}{ I, S \vdash_{jump} i \Rightarrow a_r }

\ddfrac{
    \mathbf{JumpIf} \; b \;True \mathit{off} = I[i]
    \gap
    a, \; \overbrace{\cdots}^{S^{'}} = S
}{
    b \neq \{bool(a)\}_{python}
    \gap
    I, S^{'} \vdash_{jump} \mathit{i}+1 \Rightarrow a_r
}{ I, S \vdash_{jump} i \Rightarrow a_r}

$$
\dfrac{
    \mathbf{Return} = I[i]
    \gap
    a, \; \cdots = S
}{
    I, S \vdash_{jump} i \Rightarrow a
}    
$$
$$
\dfrac{
    \mathit{instr} = I[i]
    \gap
    S \vdash_{instr} instr \Rightarrow S^{'}
    \gap
    I, S^{'} \vdash_{jump} i + 1 \Rightarrow a_r
}{
    I, S \vdash_{jump} i \Rightarrow a_r
}
$$


\newpage

\section*{Dynjit IR}

Dynjit IR can be easily translated to C language or some Python-to-C language like Cython, etc.

\subsection*{Notations and Prerequisites}

\begin{bnf*}
    \bnfprod{repr}{\bnftd{S} \bnfsp \bnftd{$\mathbf{constant}$}}\\
    \bnfmore{\bnfor \bnftd{D} \bnfsp \bnftd{$\mathbf{symbol}$}}
\end{bnf*}
(\textbf{var} is defined in the section of \textbf{Core CPY})

\begin{bnf*}
    \bnfprod{absvalue}{\bnfts{(} \bnftd{$\mathbf{repr}$} \bnfsp \bnftd{,} \bnfsp \bnftd{$\mathbf{type}$} \bnfts{)}}\\
    \bnfprod{expr}{\bnftd{$\mathbf{absvalue}$}}\\
    \bnfmore{\bnfor \bnftd{$\mathbf{expr}$} \bnfsp \bnfts{(} \bnftd{$\mathbf{expr}$} \bnfsp \bnftd{*}\bnfts{)}}\\
    \bnfprod{stmt}{\bnftd{$\mathbf{var}$} \bnfsp \bnftd{=} \bnfsp \bnftd{$\mathbf{expr}$}}\\
    \bnfmore{\bnfor \bnftd{goto} \bnfsp \bnftd{$\mbold{label}$}}\\
    \bnfmore{\bnfor \bnftd{label} \bnfsp \bnftd{$\mbold{label}$}}\\
    \bnfmore{\bnfor \bnftd{return} \bnfsp \bnftd{$\mathbf{expr}$}}\\
    \bnfmore{\bnfor \bnftd{typecheck} \bnfts{(}  \bnftd{$\mathbf{expr}$} \bnfts{,} \bnfsp \bnftd{$\mathbf{type}$} \bnfts{)} \bnfsp \bnftd{then} \bnfsp \bnftd{$\mbold{stmts}$} \bnfsp \bnftd{else} \bnfsp \bnftd{$\mbold{stmts}$}}\\
    \bnfmore{\bnfor \bnftd{if} \bnfsp \bnftd{$\mathbf{expr}$} \bnfsp \bnftd{then} \bnfsp \bnftd{$\mbold{stmts}$} \bnfsp \bnftd{else} \bnfsp \bnftd{$\mbold{stmts}$}}\\
    \bnfprod{stmts}{\bnftd{$\mathbf{stmt}$} \bnfsp \bnftd{*}}\\
\end{bnf*}
(\textbf{type} can be any Python type, such as \textit{bool}, \textit{int}, but not composite type)


\begin{itemize}
    \item $i, i_0, i_1, \cdots$, an integer.
    \item $n, n_0, n_1, \cdots$, a symbol, a name.
    \item $ct$, a function that maps a constant $a_c$ to type $ct(a_c)$.
    \item $S$, a stack of abstract values. The partial evaluation configuration state.
    \item $P$, the offset of Core CPY instructions.
    \item $G$, a map from partial evaluation configuration to the generated label in $Dynjit$.
    \item a sequence of instruction $\mathbf{I}$, can be indexed like $I[P]$.
    \item A map $\Delta$ to map a name $n$ to the index of value stack $\Delta(n)$. This is fixed for every function/instruction sequence.
    \item The side-effect-free substitution operation $\cdot[\cdot \leftarrow \cdot]$.
          Given an offset $i \in \mathbb{N}$ of the value stack, and an abstract value $v$,
          $S[i \leftarrow v]$ returns a new stack $S^{'}$ which equals to $S$ except $S[i]$ = $v$.
    \item \textit{specialize}, a function to perform partial evaluation for a function according to the argument types
\end{itemize}

Note that $G$ and $I$ does not appear in the left hand side of $\vdash$, because each function has their unique and constant $G$ and $I$.
They're used for lookup only. $G$ and $I$ can be found from \textbf{JIT STATE} when the function is known.

\subsection*{Partial Evaluation: from Core CPY to Dynjit}

\hrulefill
\bigbreak

\rulegroup{CACHE}
$$
\dfrac{
    \exists \mathit{lbl} : (S, P) \mapsto \mathit{lbl} \; \; \in G
}{
    S \vdash P \Rightarrow \mathbf{goto} \; \mathit{lbl}
}
$$

\rulegroup{LABEL}

$$
\forall \mathit{lbl} : (S, P) \mapsto \mathit{lbl} \;\; \notin G
$$

\vspace*{-\baselineskip}
$$
\dfrac{
    \mathit{lbl} = gensym()
    \gap
    G \overset{mutate}{\leftarrow} (S, P) \mapsto \mathit{lbl}, \;\;  G
    \gap
    S \vdash P \Rightarrow L
}{
    S \vdash^{*} P \Rightarrow \mathbf{label} \; \mathit{lbl} ; \; \overbrace{\cdots}^{L}
}
$$

$\vdash^{*}$ is a wrap of $\vdash$ to record generated configurations.

\newpage

\rulegroup{BOOL-SPLIT}
\vspace*{\baselineskip}


$$
(D\; n, bool), \; \overbrace{\cdots}^{S^{'}}  = S
$$

\vspace*{-1.5\baselineskip}
$$
\dfrac{
    \gap
    S_1 = (S\;True, bool), \; \overbrace{\cdots}^{S^{'}}
    \gap
    S_2 = (S\;False, bool), \; \overbrace{\cdots}^{S^{'}}
    \gap
    S_1 \vdash P \Rightarrow L_1
    \gap
    S_2 \vdash P \Rightarrow L_2
}{
    S \vdash P \Rightarrow \mathbf{if} \; (D\; n, bool) \; \mathbf{then} \; L_1 \; \mathbf{else} \; L_2
}
$$


\rulegroup{UNION-SPLIT}

$$
(repr, t_1 | t_2), \; \overbrace{\cdots}^{S^{'}}  = S
$$

\vspace*{-1.5\baselineskip}
$$
\dfrac{
    \gap
    S_1 = (repr, t_1), \; \overbrace{\cdots}^{S^{'}}
    \gap
    S_2 = (repr, t_2), \; \overbrace{\cdots}^{S^{'}}
    \gap
    S_1 \vdash P \Rightarrow L_1
    \gap
    S_2 \vdash P \Rightarrow L_2
}{
    S \vdash P \Rightarrow  \mathbf{typecheck}((repr, \top), t_1) \; \mathbf{then} \; L_1 \; \mathbf{else} \; L_2
}
$$


\rulegroup{INSTR}

\begin{equation*}
\begin{split}
\frac{
    \mathbf{Pop} = I[P] \gap
    a , \; \overbrace{\cdots}^{S^{'}} = S \gap
    S^{'} \vdash P+1 \Rightarrow L
}{
    S \vdash P \Rightarrow L
}
\\
\frac{
    \mathbf{Peek}\; i = I[P] \gap
    a_1, \cdots, a_i, a_{peek}, \cdots = S \;\;\;\;
    a_{peek}, \; \overbrace{\cdots}^{S}  \vdash P + 1 \Rightarrow L
}{
    S \vdash P \Rightarrow L
}
\\
\frac{\mathbf{Load}\; n = I[P] \gap i = \Delta(n) \gap a = S[i] \gap a, \; \overbrace{\cdots}^{S^{'}} \vdash I \Rightarrow L }
     {S \vdash P  \Rightarrow L }
\;\;
\\
\frac{ \mathbf{Const}\; a = I[P] \gap (S\;a, ct(a)), \; \overbrace{\cdots}^{S} \vdash P + 1 \Rightarrow L  }
       { S \vdash P \Rightarrow L }
\end{split}
\end{equation*}

\begin{equation*}
\begin{split}
\frac{
    \mathbf{Rot} \; i = I[P]
    \gap
    a_1, a_2, \cdots, a_i, \; \overbrace{\cdots}^{S^{'}} = S
    \gap
    a_i, a_1, a_2, \cdots, a_{i - 1},\; \overbrace{\cdots}^{S^{'}} \vdash P + 1 \Rightarrow L
}{
    S \vdash P \Rightarrow L
}
\end{split}
\end{equation*}

\ddfrac{
    \mathbf{Store} \; n = I[P]
    \gap
    i = \Delta(n)
    \gap
    (S \; a, t) ,\; \overbrace{\cdots}^{S^{'}} = S
}{
    S^{'}[i \leftarrow (S \; a, t)] \vdash P + 1 \Rightarrow L
}{
    S \vdash P \Rightarrow L
}

\ddfrac{
    \mathbf{Store} \; n = I[P]
    \gap
    i = \Delta(n)
    \gap
    (D \; n^{'}, t) ,\; \overbrace{\cdots}^{S^{'}} = S
}{
    S^{'}[i \leftarrow (D \; n^{'}, t)] \vdash P + 1 \Rightarrow L
}{
    n = (n^{'}, t); \;  \overbrace{\cdots}^{L}
}

\vspace*{\baselineskip}
$$
\mathbf{Call} \; n = I[P] \gap n = gensym()
$$
\vspace*{-\baselineskip}
$$
(r_1, t_1), (r_2, t_2) , \cdots, (r_n, t_n), (r_f, t_f), \; \overbrace{\cdots}^{S^{'}} = S
$$
\vspace*{-\baselineskip}
$$
closure, func, t_r = specialize(t_f, (t_1, t_2, \cdots, t_n))
$$
\vspace*{-\baselineskip}
$$
\dfrac{
    (D n, t_r), \overbrace{\cdots}^{S^{'}} \vdash P + 1 \Rightarrow L
}{
    S \vdash P \Rightarrow n = func (closure, (r_1, t_1), (r_2, t_2), \cdots, (r_n, t_n)) ; \; \overbrace{\cdots}^{L}
}
$$

\vspace*{\baselineskip}
\hrulefill
\vspace*{\baselineskip}

\rulegroup{JUMP}

\begin{equation*}
\begin{split}
\frac{ \mathbf{Jump} \; \mathit{off} = I[P] \;\;\;\; S \vdash^{*} \mathit{off} \Rightarrow L}
     { S \vdash P \Rightarrow L}
\end{split}
\end{equation*}


\ddfrac{
    \mathbf{JumpIf} \; b_1 \; False \; \mathit{off} = I[P]
    \gap
    (S \; b_2, bool), \; \overbrace{\cdots}^{S^{'}} = S
}{
    b_1 = b_2
    \gap
    S^{'} \vdash^{*} \mathit{off} \Rightarrow L
}{ S \vdash P \Rightarrow L }

\ddfrac{
    \mathbf{JumpIf} \; b_1 \; False \; \mathit{off} = I[P]
    \gap
    (S \; b_2, bool), \; \overbrace{\cdots}^{S^{'}} = S
}{
    b_1 \neq b_2
    \gap
    S^{'} \vdash^{*} P + 1 \Rightarrow L
}{ S \vdash P \Rightarrow L }


\ddfrac{
    \mathbf{JumpIf} \; b_1 \; True \; \mathit{off} = I[P]
    \gap
    (S \; b_2, bool), \; \cdots = S
}{
    b_1 = b_2
    \gap
    S \vdash^{*} \mathit{off} \Rightarrow L
}{ S \vdash P \Rightarrow L }

\ddfrac{
    \mathbf{JumpIf} \; b_1 \; True \; \mathit{off} = I[P]
    \gap
    (S \; b_2, bool), \; \overbrace{\cdots}^{S^{'}} = S
}{
    b_1 \neq b_2
    \gap
    S^{'} \vdash^{*} P + 1 \Rightarrow L
}{ S \vdash P \Rightarrow L }

$$
\dfrac{
    \mathbf{Return} = I[P]
    \gap
    a, \; \cdots = S
}{
    S \vdash P \Rightarrow \mathbf{return}\; a
}
$$

\medskip
\bibliographystyle{unsrt}
% \bibliography{PE2}

\end{document}

